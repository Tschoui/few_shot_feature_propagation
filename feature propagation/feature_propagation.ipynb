{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ce50f7f-32d4-47b1-af79-55960945fa9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dac08684-9033-4201-b8ce-f2c3d5b972d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a70080b-da37-45c9-9991-e47a8b74e0a0",
   "metadata": {},
   "source": [
    "**Load the toxcast dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "714d9e00-113f-4788-b708-ef7343fd766e",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_toxcast_features_train_val_test = np.load('../preprocessing/preprocessed_data/toxcast_features_train_val_test.npy', allow_pickle=True).item()\n",
    "\n",
    "loaded_toxcast_ecfp_descr_quantiles_scaled_X_train = loaded_toxcast_features_train_val_test['train']\n",
    "loaded_toxcast_ecfp_descr_quantiles_scaled_X_val = loaded_toxcast_features_train_val_test['validation']\n",
    "loaded_toxcast_ecfp_descr_quantiles_scaled_X_test = loaded_toxcast_features_train_val_test['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31bff592-b3df-4095-88f7-8a06560845b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "toxcast_features = np.concatenate((loaded_toxcast_ecfp_descr_quantiles_scaled_X_train,\n",
    "                                  loaded_toxcast_ecfp_descr_quantiles_scaled_X_val,\n",
    "                                  loaded_toxcast_ecfp_descr_quantiles_scaled_X_test), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9bf8de71-492a-42c8-834b-0e05af0447e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8595, 2248)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toxcast_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a948bb78-ea8b-4d38-8ff8-2954a80ca63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_toxcast_labels_train_val_test = np.load('../preprocessing/preprocessed_data/toxcast_labels_train_val_test.npy', allow_pickle=True).item()\n",
    "\n",
    "loaded_toxcast_y_train = loaded_toxcast_labels_train_val_test['train']\n",
    "loaded_toxcast_y_val = loaded_toxcast_labels_train_val_test['validation']\n",
    "loaded_toxcast_y_test = loaded_toxcast_labels_train_val_test['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "51db1580-da7a-408a-a3ac-d0898c2102c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "toxcast_labels = np.concatenate((loaded_toxcast_y_train,\n",
    "                                  loaded_toxcast_y_val,\n",
    "                                  loaded_toxcast_y_test), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "094ca371-7749-4e51-b13f-21d70f84ff5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8595, 617)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toxcast_labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa67bb9a-33ab-43e9-bd02-908d42f0a9f0",
   "metadata": {},
   "source": [
    "**Split the dataset into validation and test sets and create splits for each task**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "38b53e73-4f73-43ee-ad11-9aa7db1f1131",
   "metadata": {},
   "outputs": [],
   "source": [
    "toxcast_labels_val = toxcast_labels[:,:308]\n",
    "toxcast_labels_test = toxcast_labels[:,308:617]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "60012fe5-8b89-43f5-8027-1b29d3e0a644",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((8595, 308), (8595, 309))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toxcast_labels_val.shape, toxcast_labels_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3cb0c28f-5c34-40c4-af78-9f6a0904672d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def split_indices_for_task(task_labels):\n",
    "    active_indices = np.where(task_labels == 1)[0]\n",
    "    inactive_indices = np.where(task_labels == 0)[0]\n",
    "\n",
    "    np.random.seed(42)\n",
    "    selected_active = np.random.choice(active_indices, size=5, replace=False)\n",
    "    selected_inactive = np.random.choice(inactive_indices, size=5, replace=False)\n",
    "\n",
    "    rest_indices = np.setdiff1d(np.arange(task_labels.shape[0]), np.concatenate([selected_active, selected_inactive]))\n",
    "\n",
    "    return selected_active, selected_inactive, rest_indices\n",
    "\n",
    "def process_indices_for_all_tasks(toxcast_labels_val):\n",
    "    all_tasks_indices = []\n",
    "    \n",
    "    for task in range(toxcast_labels_val.shape[1]):\n",
    "        task_labels = toxcast_labels_val[:, task]\n",
    "\n",
    "        selected_active, selected_inactive, rest_indices = split_indices_for_task(task_labels)\n",
    "\n",
    "        task_indices = {\n",
    "            \"task\": task,\n",
    "            \"indices_active\": selected_active,\n",
    "            \"indices_inactive\": selected_inactive,\n",
    "            \"indices_rest\": rest_indices\n",
    "        }\n",
    "        \n",
    "        all_tasks_indices.append(task_indices)\n",
    "    \n",
    "    return all_tasks_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "15668f3d-7a93-4a95-ad9e-8ddadb7fd46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_tasks_indices = process_indices_for_all_tasks(toxcast_labels_val)\n",
    "test_tasks_indices = process_indices_for_all_tasks(toxcast_labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "734d0f02-a5fd-45e1-92bf-dda24538dcb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'task': 0,\n",
       " 'indices_active': array([6522, 2862, 1497, 7059, 1526], dtype=int64),\n",
       " 'indices_inactive': array([6568, 1766, 2715, 6455, 3518], dtype=int64),\n",
       " 'indices_rest': array([   0,    1,    2, ..., 8592, 8593, 8594])}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_tasks_indices[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f4f2fb8a-b380-4fe3-8265-25f58cf01b05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'task': 0,\n",
       " 'indices_active': array([6210, 4636, 6840, 1168, 3190], dtype=int64),\n",
       " 'indices_inactive': array([7281, 1869, 6455, 3755, 1701], dtype=int64),\n",
       " 'indices_rest': array([   0,    1,    2, ..., 8592, 8593, 8594])}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_tasks_indices[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c024d1-751c-4b21-b8ad-93b638589de2",
   "metadata": {},
   "source": [
    "**Create a new logistic regression model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1fb02a93-ed0f-4297-8229-b4d9882ee91b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_layer(in_size, out_size, activation_function=nn.ReLU, p=0.25):\n",
    "    return nn.Sequential(\n",
    "        nn.AlphaDropout(p=p),\n",
    "        nn.Linear(in_size, out_size),\n",
    "        activation_function()\n",
    "    )\n",
    "\n",
    "\n",
    "class DNN(nn.Module):\n",
    "    def __init__(self, input_layer, hidden_layers, output_layer, activation_function=nn.ReLU, p=0.25):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.input_layer = nn.Linear(input_layer, hidden_layers[0])\n",
    "        self.act1 = activation_function()\n",
    "        self.dropout1 = nn.AlphaDropout(p=p)\n",
    "\n",
    "        layers = [create_layer(hl_in, hl_out, activation_function, p) for hl_in, hl_out in zip(hidden_layers, hidden_layers[1:])]\n",
    "        \n",
    "        self.hidden_layers = nn.Sequential(*layers)\n",
    "        \n",
    "        self.output_layer = nn.Linear(hidden_layers[-1], output_layer)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.dropout1(x)\n",
    "        x = self.act1(self.input_layer(x))\n",
    "        x = self.hidden_layers(x)\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "\n",
    "params = {'input_size': 2248,\n",
    "          'hidden_layers': [1024, 128],\n",
    "          'output_size': 12,\n",
    "          'activation_function': nn.SELU,\n",
    "          'dropout_p': 0.45\n",
    "         }\n",
    "\n",
    "input_size, hidden_layers, output_size, activation_function, dropout_p = params.values()\n",
    "\n",
    "\n",
    "model = DNN(input_size, hidden_layers, output_size, activation_function, p=dropout_p)\n",
    "model.load_state_dict(torch.load('../pretraining/training/dnn_best_model.pth', map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "481f6a9b-b23c-46bc-96c8-feed3b6bfc65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DNN(\n",
       "  (input_layer): Linear(in_features=2248, out_features=1024, bias=True)\n",
       "  (act1): SELU()\n",
       "  (dropout1): AlphaDropout(p=0.45, inplace=False)\n",
       "  (hidden_layers): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): AlphaDropout(p=0.45, inplace=False)\n",
       "      (1): Linear(in_features=1024, out_features=128, bias=True)\n",
       "      (2): SELU()\n",
       "    )\n",
       "  )\n",
       "  (output_layer): Linear(in_features=128, out_features=12, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b2be285-0e79-4183-bc1d-5e0f8488564e",
   "metadata": {},
   "source": [
    "**Train & validation loop**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fcb74684-19e0-49b0-9d6f-c7e56bde6911",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_features(model, features):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        features = torch.tensor(features, dtype=torch.float32).to(device)\n",
    "        last_hl_output = model.input_layer(features)\n",
    "        last_hl_output = model.act1(last_hl_output)\n",
    "        last_hl_output = model.hidden_layers(last_hl_output)\n",
    "        last_hl_output = last_hl_output.cpu().numpy()\n",
    "    return last_hl_output\n",
    "\n",
    "def cosine_similarity_matrix(features):\n",
    "    norm_features = features / np.linalg.norm(features, axis=1, keepdims=True)\n",
    "    return np.dot(norm_features, norm_features.T)\n",
    "\n",
    "def build_graph(features, k=10):\n",
    "    cosine_sim = cosine_similarity_matrix(features)\n",
    "    adjacency_matrix = np.zeros_like(cosine_sim)\n",
    "    \n",
    "    for i in range(cosine_sim.shape[0]):\n",
    "        top_k_indices = np.argsort(-cosine_sim[i, :])[:k+1]\n",
    "        top_k_indices = top_k_indices[top_k_indices != i]\n",
    "        adjacency_matrix[i, top_k_indices] = cosine_sim[i, top_k_indices]\n",
    "\n",
    "    degree_matrix = np.diag(adjacency_matrix.sum(axis=1))\n",
    "    degree_inv_sqrt = np.diag(1 / np.sqrt(np.diag(degree_matrix) + 1e-7))\n",
    "    normalized_adjacency = np.dot(np.dot(degree_inv_sqrt, adjacency_matrix), degree_inv_sqrt)\n",
    "    \n",
    "    return normalized_adjacency\n",
    "\n",
    "def feature_propagation(features, laplacian_matrix, alpha=0.5, k=3):\n",
    "    identity_matrix = np.eye(features.shape[0])\n",
    "    diffusion_matrix = np.linalg.matrix_power((alpha * identity_matrix + laplacian_matrix), k)\n",
    "    return np.dot(diffusion_matrix, features)\n",
    "\n",
    "class LRNN(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super().__init__()\n",
    "        self.output_layer = nn.Linear(input_size, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.output_layer(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "def train_validate_task(\n",
    "    dnn_model, \n",
    "    lr_model, \n",
    "    task_indices, \n",
    "    toxcast_features, \n",
    "    toxcast_labels_val, \n",
    "    optimizer, \n",
    "    criterion, \n",
    "    k=10, \n",
    "    alpha=0.5, \n",
    "    diffusion_k=3):\n",
    "    \n",
    "    active_indices = task_indices['indices_active']\n",
    "    inactive_indices = task_indices['indices_inactive']\n",
    "    rest_indices = task_indices['indices_rest']\n",
    "    \n",
    "    train_indices = np.concatenate([active_indices, inactive_indices])\n",
    "    train_features = toxcast_features[train_indices]\n",
    "    train_labels = toxcast_labels_val[train_indices, task_indices['task']]\n",
    "    \n",
    "    val_features = toxcast_features[rest_indices]\n",
    "    val_labels = toxcast_labels_val[rest_indices, task_indices['task']]\n",
    "    valid_mask = val_labels != -1\n",
    "    val_features = val_features[valid_mask]\n",
    "    val_labels = val_labels[valid_mask]\n",
    "\n",
    "    train_features = extract_features(dnn_model, train_features)\n",
    "    val_features = extract_features(dnn_model, val_features)\n",
    "    \n",
    "    all_features = np.concatenate([train_features, val_features], axis=0)\n",
    "\n",
    "    laplacian_matrix = build_graph(all_features, k=k)\n",
    "    all_features_propagated = feature_propagation(all_features, laplacian_matrix, alpha=alpha, k=diffusion_k)\n",
    "\n",
    "    train_features_propagated = all_features_propagated[:len(train_features)]\n",
    "    val_features_propagated = all_features_propagated[len(train_features):]\n",
    "\n",
    "    train_features_propagated = torch.tensor(train_features_propagated, dtype=torch.float32).to(device)\n",
    "    train_labels = torch.tensor(train_labels, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "    \n",
    "    val_features_propagated = torch.tensor(val_features_propagated, dtype=torch.float32).to(device)\n",
    "    val_labels = torch.tensor(val_labels, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "\n",
    "    lr_model.train()\n",
    "    optimizer.zero_grad()\n",
    "    outputs = lr_model(train_features_propagated)\n",
    "    loss = criterion(outputs, train_labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    lr_model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_outputs = lr_model(val_features_propagated)\n",
    "        val_loss = criterion(val_outputs, val_labels).item()\n",
    "        val_outputs = val_outputs.cpu().numpy()\n",
    "        val_labels = val_labels.cpu().numpy()\n",
    "\n",
    "    if len(val_labels) > 0 and len(np.unique(val_labels)) > 1:\n",
    "        roc_auc = roc_auc_score(val_labels, val_outputs)\n",
    "    else:\n",
    "        roc_auc = float('nan')\n",
    "    \n",
    "    return loss.item(), val_loss, roc_auc\n",
    "\n",
    "def train_all_tasks(dnn_model, toxcast_features, toxcast_labels_val, val_tasks_indices, num_epochs=10, learning_rate=0.001, patience=3, k=10, alpha=0.5, diffusion_k=3):\n",
    "    all_train_losses_per_epoch = []\n",
    "    all_val_losses_per_epoch = []\n",
    "    mean_roc_aucs_per_epoch = []\n",
    "    \n",
    "    best_mean_roc_auc = float('-inf')\n",
    "    best_epoch = -1\n",
    "    no_improvement_counter = 0\n",
    "    \n",
    "    best_models = {}\n",
    "\n",
    "    models = {}\n",
    "    optimizers = {}\n",
    "    criterions = {}\n",
    "\n",
    "    for task_indices in val_tasks_indices:\n",
    "        input_size = hidden_layers[-1]\n",
    "        task_model = LRNN(input_size)\n",
    "        task_model.to(device)\n",
    "        optimizer = optim.Adam(task_model.parameters(), lr=learning_rate)\n",
    "        \n",
    "        models[task_indices['task']] = task_model\n",
    "        optimizers[task_indices['task']] = optimizer\n",
    "        criterions[task_indices['task']] = nn.BCELoss()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_train_losses = []\n",
    "        epoch_val_losses = []\n",
    "        epoch_roc_aucs = []\n",
    "        total_train_loss = 0\n",
    "        total_val_loss = 0\n",
    "        \n",
    "        for task_indices in val_tasks_indices:\n",
    "            task_model = models[task_indices['task']]\n",
    "            optimizer = optimizers[task_indices['task']]\n",
    "            criterion = criterions[task_indices['task']]\n",
    "            \n",
    "            train_loss, val_loss, roc_auc = train_validate_task(\n",
    "                dnn_model, task_model, task_indices, toxcast_features, toxcast_labels_val, optimizer, criterion, k=k, alpha=alpha, diffusion_k=diffusion_k\n",
    "            )\n",
    "            \n",
    "            epoch_train_losses.append(train_loss)\n",
    "            epoch_val_losses.append(val_loss)\n",
    "            epoch_roc_aucs.append(roc_auc)\n",
    "            total_train_loss += train_loss\n",
    "            total_val_loss += val_loss\n",
    "        \n",
    "        mean_roc_auc = np.nanmean(epoch_roc_aucs)\n",
    "        \n",
    "        all_train_losses_per_epoch.append(total_train_loss)\n",
    "        all_val_losses_per_epoch.append(total_val_loss)\n",
    "        mean_roc_aucs_per_epoch.append(mean_roc_auc)\n",
    "        \n",
    "        if mean_roc_auc > best_mean_roc_auc:\n",
    "            best_mean_roc_auc = mean_roc_auc\n",
    "            best_epoch = epoch\n",
    "            no_improvement_counter = 0\n",
    "\n",
    "            for task_indices in val_tasks_indices:\n",
    "                best_models[task_indices[\"task\"]] = models[task_indices['task']].state_dict()\n",
    "        else:\n",
    "            no_improvement_counter += 1\n",
    "        \n",
    "        if no_improvement_counter >= patience:\n",
    "            print(f\"Early stopping after {epoch + 1} epochs. Best AUC: {best_mean_roc_auc:.4f} at epoch {best_epoch + 1}\")\n",
    "            break\n",
    "                \n",
    "        print(f'Epoch {epoch + 1}/{num_epochs} - Mean ROC AUC: {mean_roc_auc:.4f}, '\n",
    "              f'Total Train Loss: {total_train_loss:.4f}, ' \n",
    "              f'Total Validation Loss: {total_val_loss:.4f}, '\n",
    "              f'Best AUC: {best_mean_roc_auc}')\n",
    "    \n",
    "    torch.save(best_models, 'best_models.pth')\n",
    "    \n",
    "    return all_train_losses_per_epoch, all_val_losses_per_epoch, mean_roc_aucs_per_epoch, best_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "337c6588-42a6-4682-aa15-08215365e85d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 - Mean ROC AUC: 0.5017, Total Train Loss: 374.7683, Total Validation Loss: 367.7821, Best AUC: 0.5016561024413451\n",
      "Epoch 2/10 - Mean ROC AUC: 0.5100, Total Train Loss: 345.6468, Total Validation Loss: 345.7915, Best AUC: 0.5099528154041377\n",
      "Epoch 3/10 - Mean ROC AUC: 0.5182, Total Train Loss: 319.0021, Total Validation Loss: 326.4700, Best AUC: 0.5181822341239953\n",
      "Epoch 4/10 - Mean ROC AUC: 0.5264, Total Train Loss: 294.8560, Total Validation Loss: 309.7935, Best AUC: 0.5264124974426249\n",
      "Epoch 5/10 - Mean ROC AUC: 0.5344, Total Train Loss: 273.1644, Total Validation Loss: 295.6478, Best AUC: 0.5344176505220728\n",
      "Epoch 6/10 - Mean ROC AUC: 0.5424, Total Train Loss: 253.8270, Total Validation Loss: 283.8645, Best AUC: 0.5424352702662786\n",
      "Epoch 7/10 - Mean ROC AUC: 0.5504, Total Train Loss: 236.6990, Total Validation Loss: 274.2456, Best AUC: 0.5503604334733173\n",
      "Epoch 8/10 - Mean ROC AUC: 0.5579, Total Train Loss: 221.6016, Total Validation Loss: 266.5712, Best AUC: 0.5579036262909991\n",
      "Epoch 9/10 - Mean ROC AUC: 0.5652, Total Train Loss: 208.3299, Total Validation Loss: 260.6042, Best AUC: 0.5651987954820902\n",
      "Epoch 10/10 - Mean ROC AUC: 0.5722, Total Train Loss: 196.6618, Total Validation Loss: 256.0982, Best AUC: 0.5721732302644247\n"
     ]
    }
   ],
   "source": [
    "train_losses_per_epoch, val_losses_per_epoch, mean_roc_aucs_per_epoch, best_epoch = train_all_tasks(\n",
    "    model, \n",
    "    toxcast_features, \n",
    "    toxcast_labels_val, \n",
    "    val_tasks_indices, \n",
    "    num_epochs=10, \n",
    "    learning_rate=1e-3, \n",
    "    patience=5, \n",
    "    k=5, \n",
    "    alpha=0.5, \n",
    "    diffusion_k=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f6c12ef-78ee-4b5c-88c6-dafdc0b43583",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
