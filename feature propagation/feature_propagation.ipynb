{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ce50f7f-32d4-47b1-af79-55960945fa9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "714d9e00-113f-4788-b708-ef7343fd766e",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_toxcast_features_train_val_test = np.load('../preprocessing/preprocessed_data/toxcast_features_train_val_test.npy', allow_pickle=True).item()\n",
    "\n",
    "loaded_toxcast_ecfp_descr_quantiles_scaled_X_train = loaded_toxcast_features_train_val_test['train']\n",
    "loaded_toxcast_ecfp_descr_quantiles_scaled_X_val = loaded_toxcast_features_train_val_test['validation']\n",
    "loaded_toxcast_ecfp_descr_quantiles_scaled_X_test = loaded_toxcast_features_train_val_test['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "31bff592-b3df-4095-88f7-8a06560845b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "toxcast_features = np.concatenate((loaded_toxcast_ecfp_descr_quantiles_scaled_X_train,\n",
    "                                  loaded_toxcast_ecfp_descr_quantiles_scaled_X_val,\n",
    "                                  loaded_toxcast_ecfp_descr_quantiles_scaled_X_test), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9bf8de71-492a-42c8-834b-0e05af0447e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8595, 2248)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toxcast_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a948bb78-ea8b-4d38-8ff8-2954a80ca63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_toxcast_labels_train_val_test = np.load('../preprocessing/preprocessed_data/toxcast_labels_train_val_test.npy', allow_pickle=True).item()\n",
    "\n",
    "loaded_toxcast_y_train = loaded_toxcast_labels_train_val_test['train']\n",
    "loaded_toxcast_y_val = loaded_toxcast_labels_train_val_test['validation']\n",
    "loaded_toxcast_y_test = loaded_toxcast_labels_train_val_test['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "51db1580-da7a-408a-a3ac-d0898c2102c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "toxcast_labels = np.concatenate((loaded_toxcast_y_train,\n",
    "                                  loaded_toxcast_y_val,\n",
    "                                  loaded_toxcast_y_test), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "094ca371-7749-4e51-b13f-21d70f84ff5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8595, 617)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toxcast_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "38b53e73-4f73-43ee-ad11-9aa7db1f1131",
   "metadata": {},
   "outputs": [],
   "source": [
    "toxcast_labels_val = toxcast_labels[:,:308]\n",
    "toxcast_labels_test = toxcast_labels[:,308:617]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "60012fe5-8b89-43f5-8027-1b29d3e0a644",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((8595, 308), (8595, 309))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toxcast_labels_val.shape, toxcast_labels_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3cb0c28f-5c34-40c4-af78-9f6a0904672d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def split_indices_for_task(task_labels):\n",
    "    active_indices = np.where(task_labels == 1)[0]\n",
    "    inactive_indices = np.where(task_labels == 0)[0]\n",
    "\n",
    "    np.random.seed(42)\n",
    "    selected_active = np.random.choice(active_indices, size=5, replace=False)\n",
    "    selected_inactive = np.random.choice(inactive_indices, size=5, replace=False)\n",
    "\n",
    "    rest_indices = np.setdiff1d(np.arange(task_labels.shape[0]), np.concatenate([selected_active, selected_inactive]))\n",
    "\n",
    "    return selected_active, selected_inactive, rest_indices\n",
    "\n",
    "def process_indices_for_all_tasks(toxcast_labels_val):\n",
    "    all_tasks_indices = []\n",
    "    \n",
    "    for task in range(toxcast_labels_val.shape[1]):\n",
    "        task_labels = toxcast_labels_val[:, task]\n",
    "\n",
    "        selected_active, selected_inactive, rest_indices = split_indices_for_task(task_labels)\n",
    "\n",
    "        task_indices = {\n",
    "            \"task\": task,\n",
    "            \"indices_active\": selected_active,\n",
    "            \"indices_inactive\": selected_inactive,\n",
    "            \"indices_rest\": rest_indices\n",
    "        }\n",
    "        \n",
    "        all_tasks_indices.append(task_indices)\n",
    "    \n",
    "    return all_tasks_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "15668f3d-7a93-4a95-ad9e-8ddadb7fd46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_tasks_indices = process_indices_for_all_tasks(toxcast_labels_val)\n",
    "test_tasks_indices = process_indices_for_all_tasks(toxcast_labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "734d0f02-a5fd-45e1-92bf-dda24538dcb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'task': 0,\n",
       " 'indices_active': array([6522, 2862, 1497, 7059, 1526], dtype=int64),\n",
       " 'indices_inactive': array([6568, 1766, 2715, 6455, 3518], dtype=int64),\n",
       " 'indices_rest': array([   0,    1,    2, ..., 8592, 8593, 8594])}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_tasks_indices[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f4f2fb8a-b380-4fe3-8265-25f58cf01b05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'task': 0,\n",
       " 'indices_active': array([6210, 4636, 6840, 1168, 3190], dtype=int64),\n",
       " 'indices_inactive': array([7281, 1869, 6455, 3755, 1701], dtype=int64),\n",
       " 'indices_rest': array([   0,    1,    2, ..., 8592, 8593, 8594])}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_tasks_indices[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1fb02a93-ed0f-4297-8229-b4d9882ee91b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_layer(in_size, out_size, activation_function=nn.ReLU, p=0.25):\n",
    "    return nn.Sequential(\n",
    "        nn.AlphaDropout(p=p),\n",
    "        nn.Linear(in_size, out_size),\n",
    "        activation_function()\n",
    "    )\n",
    "\n",
    "\n",
    "class DNN(nn.Module):\n",
    "    def __init__(self, input_layer, hidden_layers, output_layer, activation_function=nn.ReLU, p=0.25):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.input_layer = nn.Linear(input_layer, hidden_layers[0])\n",
    "        self.act1 = activation_function()\n",
    "        self.dropout1 = nn.AlphaDropout(p=p)\n",
    "\n",
    "        layers = [create_layer(hl_in, hl_out, activation_function, p) for hl_in, hl_out in zip(hidden_layers, hidden_layers[1:])]\n",
    "        \n",
    "        self.hidden_layers = nn.Sequential(*layers)\n",
    "        \n",
    "        self.output_layer = nn.Linear(hidden_layers[-1], output_layer)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.dropout1(x)\n",
    "        x = self.act1(self.input_layer(x))\n",
    "        x = self.hidden_layers(x)\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "\n",
    "class LRNN(nn.Module):\n",
    "    def __init__(self, feature_extractor, output_size=1):\n",
    "        super().__init__()\n",
    "        self.input_layer = feature_extractor.input_layer\n",
    "        self.act1 = feature_extractor.act1\n",
    "        self.dropout1 = feature_extractor.dropout1\n",
    "        self.hidden_layers = feature_extractor.hidden_layers\n",
    "\n",
    "        self.output_layer2 = nn.Linear(feature_extractor.hidden_layers[-1][1].out_features, output_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.dropout1(x)\n",
    "        x = self.act1(self.input_layer(x))\n",
    "        x = self.hidden_layers(x)\n",
    "        x = self.output_layer2(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "params = {'input_size': 2248,\n",
    "          'hidden_layers': [1024, 128],\n",
    "          'output_size': 12,\n",
    "          'activation_function': nn.SELU,\n",
    "          'dropout_p': 0.45\n",
    "         }\n",
    "\n",
    "input_size, hidden_layers, output_size, activation_function, dropout_p = params.values()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = DNN(input_size, hidden_layers, output_size, activation_function, p=dropout_p)\n",
    "model.load_state_dict(torch.load('../pretraining/training/dnn_best_model.pth', map_location=device))\n",
    "model.eval()\n",
    "\n",
    "lr_model = LRNN(model)\n",
    "lr_model.to(device)\n",
    "\n",
    "model_dict = model.state_dict()\n",
    "lr_model_dict = lr_model.state_dict()\n",
    "\n",
    "filtered_dict = {k: v for k, v in model_dict.items() if k in lr_model_dict and 'output_layer' not in k}\n",
    "\n",
    "lr_model_dict.update(filtered_dict)\n",
    "lr_model.load_state_dict(lr_model_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "74241958-ac20-430f-b63d-6d566299f364",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LRNN(\n",
       "  (input_layer): Linear(in_features=2248, out_features=1024, bias=True)\n",
       "  (act1): SELU()\n",
       "  (dropout1): AlphaDropout(p=0.45, inplace=False)\n",
       "  (hidden_layers): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): AlphaDropout(p=0.45, inplace=False)\n",
       "      (1): Linear(in_features=1024, out_features=128, bias=True)\n",
       "      (2): SELU()\n",
       "    )\n",
       "  )\n",
       "  (output_layer2): Linear(in_features=128, out_features=1, bias=True)\n",
       "  (sigmoid): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fcb74684-19e0-49b0-9d6f-c7e56bde6911",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import numpy as np\n",
    "\n",
    "def train_validate_task(lr_model, task_indices, toxcast_features, toxcast_labels_val, optimizer, criterion):\n",
    "    active_indices = task_indices['indices_active']\n",
    "    inactive_indices = task_indices['indices_inactive']\n",
    "    rest_indices = task_indices['indices_rest']\n",
    "    \n",
    "    train_features = toxcast_features[np.concatenate([active_indices, inactive_indices])]\n",
    "    train_labels = toxcast_labels_val[np.concatenate([active_indices, inactive_indices]), task_indices['task']]\n",
    "    \n",
    "    train_features = torch.tensor(train_features, dtype=torch.float32).to(device)\n",
    "    train_labels = torch.tensor(train_labels, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "    \n",
    "    valid_mask = toxcast_labels_val[rest_indices, task_indices['task']] != -1\n",
    "    val_features = toxcast_features[rest_indices][valid_mask]\n",
    "    val_labels = toxcast_labels_val[rest_indices, task_indices['task']][valid_mask]\n",
    "    \n",
    "    val_features = torch.tensor(val_features, dtype=torch.float32).to(device)\n",
    "    val_labels = torch.tensor(val_labels, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "    \n",
    "    train_loader = DataLoader(TensorDataset(train_features, train_labels), batch_size=len(train_labels), shuffle=True)\n",
    "    val_loader = DataLoader(TensorDataset(val_features, val_labels), batch_size=len(val_labels), shuffle=False)\n",
    "    \n",
    "    lr_model.train()\n",
    "    running_loss = 0\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = lr_model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    train_loss = running_loss / len(train_loader)\n",
    "    \n",
    "    lr_model.eval()\n",
    "    val_loss = 0\n",
    "    all_labels = []\n",
    "    all_outputs = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            outputs = lr_model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            all_labels.append(labels.cpu().numpy())\n",
    "            all_outputs.append(outputs.cpu().numpy())\n",
    "    \n",
    "    val_loss = val_loss / len(val_loader)\n",
    "    \n",
    "    if len(all_labels) > 0 and len(np.unique(np.concatenate(all_labels))) > 1:\n",
    "        roc_auc = roc_auc_score(np.concatenate(all_labels), np.concatenate(all_outputs))\n",
    "    else:\n",
    "        roc_auc = float('nan')\n",
    "    \n",
    "    return train_loss, val_loss, roc_auc\n",
    "\n",
    "def train_all_tasks(toxcast_features, toxcast_labels_val, val_tasks_indices, num_epochs=10, learning_rate=0.001, patience=3):\n",
    "    all_train_losses_per_epoch = []\n",
    "    all_val_losses_per_epoch = []\n",
    "    mean_roc_aucs_per_epoch = []\n",
    "    \n",
    "    best_mean_roc_auc = float('-inf')\n",
    "    best_epoch = -1\n",
    "    no_improvement_counter = 0\n",
    "    \n",
    "    best_models = {}\n",
    "\n",
    "    models = {}\n",
    "    optimizers = {}\n",
    "    for task_indices in val_tasks_indices:\n",
    "        task_model = LRNN(model)\n",
    "        task_model.to(device)\n",
    "        \n",
    "        for param in task_model.parameters():\n",
    "            param.requires_grad = False\n",
    "        task_model.output_layer2.weight.requires_grad = True\n",
    "        task_model.output_layer2.bias.requires_grad = True\n",
    "        \n",
    "        task_model_dict = task_model.state_dict()\n",
    "        filtered_dict = {k: v for k, v in model.state_dict().items() if k in task_model_dict and 'output_layer2' not in k}\n",
    "        task_model_dict.update(filtered_dict)\n",
    "        task_model.load_state_dict(task_model_dict)\n",
    "        \n",
    "        models[task_indices['task']] = task_model\n",
    "        optimizers[task_indices['task']] = optim.Adam(task_model.output_layer2.parameters(), lr=learning_rate)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_train_losses = []\n",
    "        epoch_val_losses = []\n",
    "        epoch_roc_aucs = []\n",
    "        total_train_loss = 0\n",
    "        total_val_loss = 0\n",
    "        \n",
    "        for task_indices in val_tasks_indices:\n",
    "            task_model = models[task_indices['task']]\n",
    "            optimizer = optimizers[task_indices['task']]\n",
    "            \n",
    "            criterion = nn.BCELoss()\n",
    "            \n",
    "            train_loss, val_loss, roc_auc = train_validate_task(\n",
    "                task_model, task_indices, toxcast_features, toxcast_labels_val, optimizer, criterion\n",
    "            )\n",
    "            \n",
    "            epoch_train_losses.append(train_loss)\n",
    "            epoch_val_losses.append(val_loss)\n",
    "            epoch_roc_aucs.append(roc_auc)\n",
    "            total_train_loss += train_loss\n",
    "            total_val_loss += val_loss\n",
    "        \n",
    "        mean_roc_auc = np.nanmean(epoch_roc_aucs)\n",
    "        \n",
    "        all_train_losses_per_epoch.append(total_train_loss)\n",
    "        all_val_losses_per_epoch.append(total_val_loss)\n",
    "        mean_roc_aucs_per_epoch.append(mean_roc_auc)\n",
    "        \n",
    "        if mean_roc_auc > best_mean_roc_auc:\n",
    "            best_mean_roc_auc = mean_roc_auc\n",
    "            best_epoch = epoch\n",
    "            no_improvement_counter = 0\n",
    "\n",
    "            for task_indices in val_tasks_indices:\n",
    "                best_models[task_indices[\"task\"]] = models[task_indices['task']].output_layer2.state_dict()\n",
    "        else:\n",
    "            no_improvement_counter += 1\n",
    "        \n",
    "        if no_improvement_counter >= patience:\n",
    "            print(f\"Early stopping after {epoch + 1} epochs. Best AUC: {best_mean_roc_auc:.4f} at epoch {best_epoch + 1}\")\n",
    "            break\n",
    "                \n",
    "        print(f'Epoch {epoch + 1}/{num_epochs} - Mean ROC AUC: {mean_roc_auc:.4f}, '\n",
    "              f'Total Train Loss: {total_train_loss:.4f}, ' \n",
    "              f'Total Validation Loss: {total_val_loss:.4f}, '\n",
    "              f'Best AUC: {best_mean_roc_auc}')\n",
    "    \n",
    "    torch.save(best_models, 'best_models.pth')\n",
    "    \n",
    "    return all_train_losses_per_epoch, all_val_losses_per_epoch, mean_roc_aucs_per_epoch, best_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "337c6588-42a6-4682-aa15-08215365e85d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2 - Mean ROC AUC: 0.5029, Total Train Loss: 239.3898, Total Validation Loss: 283.6049, Best AUC: 0.5029405207289248\n",
      "Epoch 2/2 - Mean ROC AUC: 0.5098, Total Train Loss: 234.7578, Total Validation Loss: 274.9234, Best AUC: 0.5098497577904332\n"
     ]
    }
   ],
   "source": [
    "train_losses_per_epoch, val_losses_per_epoch, mean_roc_aucs_per_epoch, best_epoch = train_all_tasks(\n",
    "    toxcast_features, toxcast_labels_val, val_tasks_indices, num_epochs=2, learning_rate=1e-3, patience=5\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
